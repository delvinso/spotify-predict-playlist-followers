---
title: "03. Predicting the Number of Followers a Spotify Playlist Will Garner"
author: "Delvin So"
date: '2019-01-15'
output: 
  html_document:
    keep_md: true
    theme: journal
    toc: true
    toc_float : true
    toc_depth : 4
    df_print: tibble
  html_notebook:
editor_options: 
  chunk_output_type: inline
---

# Introduction

Please refer to [notebook]() for how the introduction and how the data was gathered, and [notebook]() for how the EDA and feature engineering.



To recap, in the previous notebook, we engineered several features based on the hypothesis that they would help in explaining the number of followers a playlist would receive. The features were based on:

Numerous features were created, mostly by aggregating onto the playlist level as follows:

1.  Total Number of Tracks in the Playlist **(1)**
2.  Artist Followers, Artist Popularity and Track Popularity  - Mean and Standard Deviation (except for artist followers) **(5)**
3. Two features were created based on the occurrence of frequently occurring (top 1%) artists and (top 5%) genres within ‘highly successful playlists’ determined as having the top 33% of playlist followers
	* Artists **(99)**
		* This was engineered by taking a list of artists appearing in playlists (of the top 33% playlists), tallying the  occurrence of artists and then looking at the top 5% occurrence. These  artists were hypothesized  to be important in determining the number of playlist followers a playlist garners, and one-hot encoded. 
	* Genres **(49)**
		*  Spotify does not have genre information on the song level, however it does provide this information on the artist level.  This was engineered by taking a list of associated genres with each unique artist in a playlist (of the top 33% playlists), tallying the genres and then looking at the top 5% occurrence of genres. These genres were hypothesized  to be important in determining the number of playlist followers a playlist garners, and one-hot encoded. 
	* To prevent leakage of training into test data, this was based solely on the training data
4. Quantitative Audio Features - 
	* Mean **(10)** & Standard Deviation **(10)**
	* Pairwise Interaction Terms **(190)**, 
	* Second Degree Polynomials **(20)**
5.  Categorical Audio Features - Key, Mode, Key Mode **(Majority Vote; 37)**

Thus, there are a total of 421 predictors over 1462 playlists.

For more in depth analysis of the models here, please refer to accompanying markdown for this project over [on my website](https://delvinso.github.io/project/2019-01-19-spotify-predict-success/).

## Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      tidy = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      cache = TRUE)
library(caret)
library(tidyverse)
library(scales)
library(naniar)
library(patchwork)
source("statSmoothFunc.R")

theme_set(theme_minimal())

options(tibble.max_extra_cols = 5) # prints only 5 columns extra to prevent wrapping around page when the tibbles get larger

```

## Helpers
```{r}

histoPlot <- function(...){
  ggplot(...) + 
    geom_histogram() +
    theme_minimal() +
  theme(strip.text = element_text(face = "bold"),
        axis.text = element_text(face = "bold"),
        axis.title = element_text(face = "bold"))
}

scttrCor <- function(...){
ggplot(...) +
  geom_point( alpha = 0.5, size = 0.8) +
  geom_smooth(formula = y~x, method = "lm", se = FALSE, size = 0.3, alpha = 0.5, colour = "black") + 
  stat_smooth_func(geom = "text", method = "lm", xpos = -Inf, ypos = Inf, hjust = -0.2, vjust = 1, parse = TRUE, size = 3) +
  theme_minimal() + 
  theme(strip.text = element_text(face = "bold"),
        axis.text = element_text(face = "bold"),
        axis.title = element_text(face = "bold"))
}

# define a function to simplify hyperparameter tuning and returning cv results for all of the above training datasets
lasso_fit_n_summarize <- function(training_x, training_y, testing_x, testing_y, type = "", verbose = FALSE){
  
  
  # initialize the grid, large values for a warm start
  # e.grid <- 10^seq(10, -2, length = 100)
  e.grid <- seq(0.001, 0.1, by = 0.0005)
  
  caret_ctrl <- trainControl(
    method = "cv",
    number = 5,
    verboseIter = verbose,
    savePredictions = TRUE
  )
  
  set.seed(1993-11-22)
  
  model_cv <- train(x = training_x,
                    y = training_y,
                    method = "glmnet",
                    trControl = caret_ctrl,
                    preProc = c("center", "scale"),

                    tuneGrid = expand.grid(alpha = c(1), lambda = e.grid))
  
  
  # cv metrics
  
  cv_metrics <- model_cv$results[model_cv$results$alpha == model_cv$bestTune$alpha & model_cv$results$lambda == model_cv$bestTune$lambda, c(1:5)]
  
  cv_metrics$set <- "cross-validation"
  
  
  lasso_preds <- predict(model_cv, (testing_x))
  
  test_metrics <- postResample(pred = lasso_preds, obs = testing_y)
  test_metrics$set <- "test"
  
  metrics <- cv_metrics %>% mutate(type = type)
  
  return(metrics)
  
}

```



## Read in Cleaned Data

```{r}
dat2 <- read_csv("output/dat.csv")
dim(dat2)
dat2
```

We need to split up our data into the same training and testing partitions (80/20) as we did for EDA. To do this we use the same seed and playlist dataframe just prior to the eda.


```{r}
# read in previously saved training data for eda to re-sample the playlist names
pl_dat <- read_csv("data/clean_pl_eda_train.csv")

set.seed(11-22-1993)

pl_names <- unique(pl_dat$playlist_name)
length(pl_names)

# retrieve a random sample of indices corresponding to the vector of playlist names
trainIndex <- sample(1:length(pl_names), size = length(pl_names) * 0.8)
# creating the training set based on the randomly sampled indices
train_dat <- filter(dat2, playlist_name %in% pl_names[trainIndex])
# creating the testing set based on the randomly sampled indices
test_dat <- dat2 %>% filter(!playlist_name %in% pl_names[trainIndex])

length(unique(train_dat$playlist_name));length(unique(test_dat$playlist_name))

```

We then partition our training and test split into the predictors (x) and the response (y).

  
```{r}

train_y <- train_dat$log_pl_followers
train_x <- select(train_dat, -c(playlist_name, playlist_followers, log_pl_followers)) 

test_y <- test_dat$log_pl_followers
test_x <- select(test_dat, -c(playlist_name, playlist_followers, log_pl_followers)) 

# quick sanity check
dim(train_x); dim(test_x)
length(train_y); length(test_y)
```

Look at the distribution of training and testing responses to see if all is well. 
```{r}

data.frame(obs = train_y) %>%
  histoPlot(aes(x = obs, fill = ..count..)) +
  scale_fill_viridis_c() + 
  ggtitle("Distribution of Training Log # of Followers")

data.frame(obs = test_y) %>% 
  histoPlot(aes(x = obs, fill = ..count..)) + 
  scale_fill_viridis_c(option = "magma") +
  ggtitle("Distribution of Testing Log # of Followers")

# convert to data matrix for modelling
train_x <- data.matrix(train_x)
test_x <- data.matrix(test_x)

```

Identify most highly correlated predictors with the log # of playlist followers.
```{r}
high_cor_preds <- train_dat %>% 
  as_tibble() %>% 
  mutate(log_playlist_followers = log(playlist_followers)) %>% 
  select(log_playlist_followers, playlist_num_tracks:tempo_sd_x_duration_ms_sd) %>%
  cor(use = "pairwise.complete.obs") %>%
  # convert to dataframe so we can retrieve the rownames, or predictors
  data.frame() %>%
  rownames_to_column("preds") %>% 
  # select only predictor names and their correlation with log playlist followers
  select(preds, log_playlist_followers) %>%
  # convert to tibble 
  as_tibble() %>%
  arrange(-abs(log_playlist_followers))

high_cor_preds
```

## Baseline Modeling - OLS 

Only popularity and follower means (3).
```{r}
# only track and popularity means, follower 
pop_train_x <- train_x %>% as_tibble() %>% select(track_popularity_mean, artist_pop_mean, artist_followers_mean)
pop_test_x <- test_x  %>% as_tibble() %>% select(track_popularity_mean, artist_pop_mean, artist_followers_mean)

# fit model
spotify_lm <- lm(data = data.frame(train_y, pop_train_x), train_y ~ . )

summary(spotify_lm)

# coefficients
broom::tidy(spotify_lm)


postResample(spotify_lm$fitted.values, obs = train_y)

ols_cv_p <- tibble(preds = spotify_lm$fitted.values, obs = train_y) %>%
  scttrCor(aes(x = preds, y = obs), alpha = 0.8) + 
    scale_x_continuous(breaks = seq(0, 17.5, by = 2.5), limits = c(0, 17.5)) + 
  scale_y_continuous(breaks = seq(0, 17.5, by = 2.5), limits = c(0, 17.5)) +
  ggtitle("OLS Model - (Baseline model)") 

ols_cv_p

```

```{r, eval = FALSE, include = FALSE}
# validation plot

# validation metrics
ols_preds <- predict(spotify_lm, newdata = data.frame(pop_test_x))
postResample(ols_preds, test_y)
ols_val_p <- tibble(preds = ols_preds, obs = test_y) %>%
  scttrCor(aes(x = preds, y = obs), alpha = 0.8) +
    scale_x_continuous(breaks = seq(0, 17.5, by = 2.5), limits = c(0, 17.5)) + 
  scale_y_continuous(breaks = seq(0, 17.5, by = 2.5), limits = c(0, 17.5)) +
  ggtitle("OLS Validation (Baseline model)") 

ols_val_p

```

## Random Forest

Let's stick with the model straight out of the box. 
```{r}
library(randomForest)
set.seed(11-22-1993)
train_rf <- randomForest(y = train_y, 
                         x = train_x, 
                   ntree = 500,
                   importance = TRUE,
                   na.rm = TRUE)
# plot(train_rf)
# train_rf

```

Variable importance is computed as (taken from :https://stats.stackexchange.com/questions/162465/in-a-random-forest-is-larger-incmse-better-or-worse)
1. grow regression forest. Compute OOB-mse, name this mse0.
2. for 1 to j var: permute values of column j, then predict and compute OOB-mse(j)
3. %IncMSE of j'th is (mse(j)-mse0)/mse0 * 100%

```{r}
rf_imp <- data.frame(importance(train_rf)) %>% rownames_to_column("preds") %>% arrange(desc(X.IncMSE))

ggplot(rf_imp[1:30, ],
       aes(x = reorder(preds, X.IncMSE), y = X.IncMSE, fill = X.IncMSE)) +
  geom_col() + 
  labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + 
  coord_flip() + 
  theme(legend.position="none") +
  ggtitle("Random Forest Variable Importance")

```

We see that the number of tracks and popularity related features are the most important variables, that is, when these variables are randomly shuffled (permuted), they result in the greatest increase of the test error. As we saw in the EDA, the track popularity is an important variable and thus likely an important characteristic of determining the success of a playlist. We also see that the majority of the top 30 most important variables are summary statistics related to quantitative audio features, with a few genres and one artist.

Model performance

```{r}
# OOB performance

# which.min(train_rf$mse)
train_rf$rsq[which.min(train_rf$mse)]
sqrt(train_rf$mse[which.min(train_rf$mse)])

# scatter plot of predicted vs observed values

# postResample(train_rf$predicted, train_y) 
rf_cv_p <- tibble(preds = train_rf$predicted, obs = train_y) %>%
  scttrCor(aes(x = preds, y = obs)) + 
    scale_x_continuous(breaks = seq(0, 17.5, by = 2.5), limits = c(0, 17.5)) + 
  scale_y_continuous(breaks = seq(0, 17.5, by = 2.5), limits = c(0, 17.5)) +
  ggtitle("Random Forest Out of Bag Model")

rf_cv_p

```

```{r, eval = FALSE, include = FALSE}
rf_preds <- predict(train_rf, test_x)
postResample(rf_preds, test_y)

rf_val_p <- tibble(preds = rf_preds, obs = test_y) %>%
scttrCor(aes(x = preds, y = obs), alpha = 0.7) +
 ggtitle("Random Forest Validation")

 rf_val_p
```

## Regularized Regression

All predictors were normalized (ie. mean = 0 and variance = 1) to allow the penalization scheme to fairly affect all predictors. 

For lasso, I decided to fit the model first using only the mean track and artist popularity, and the mean artist followers. I then add in predictors as follows:

* Quantitative Audio Features (20)
* Top 50 Predictors Correlated with the Log of playlist followers
* Top 50 Predictors + One-Hot Encoded Categorical Audio Features 
* All Predictors

```{r}

# only track and popularity means, follower 
pop_train_x <- train_x %>%
  as_tibble() %>% 
  select(track_popularity_mean, artist_pop_mean, artist_followers_mean)
pop_test_x <- test_x  %>%
  as_tibble() %>%
  select(track_popularity_mean, artist_pop_mean, artist_followers_mean)

# track and artist popularity 
quant_train_x <- train_x %>% 
  as_tibble() %>%
  select(track_popularity_mean, artist_pop_mean, danceability_avg:duration_ms_sd)
quant_test_x <- test_x %>%
  as_tibble() %>%
  select(track_popularity_mean, artist_pop_mean, danceability_avg:duration_ms_sd)

# only 'highly' correlated predictors
top_cor_train_x <- train_x %>%
  as_tibble() %>% 
  select(high_cor_preds[2:51,]$preds)
top_cor_test_x <- test_x %>%
  as_tibble() %>% 
  select(high_cor_preds[2:51,]$preds)

# highly correlated perdictors and categorical predictors
top_cor_cat_train_x <- train_x %>% 
  as_tibble() %>% 
  select(high_cor_preds[2:51,]$preds, is_major_mode:vapor_soul)
top_cor_cat_test_x <- test_x %>%
  as_tibble() %>% 
  select(high_cor_preds[2:51,]$preds, is_major_mode:vapor_soul)


# fit individually
# lasso_fit_n_summarize(pop_train_x, train_y, pop_test_x, test_y, "TEST") 
# 
# lasso_fit_n_summarize(pop_train_x, train_y, test_y, "TEST") 
# 
# lasso_fit_n_summarize(top_cor_cat_train_x, train_y, test_y, "TEST") 


# fit by iterating through a grid

lasso_list <- list(train = list(pop_train_x, quant_train_x, top_cor_train_x, top_cor_cat_train_x, train_x),
                  test = list( pop_test_x, quant_test_x, top_cor_test_x, top_cor_cat_test_x, test_x),
                  name = list("Popularity and Follower Predictors", "Quantitative Predictors", "Top 50 Predictors", "Top 50 Predictors + Categorical Audio Features", "All Predictors"))


lasso_res <- c(1:5) %>% map_dfr(function(this_function){
  
  names <- lasso_list$name[[this_function]][[1]]
  
  print(paste("Cross-Validating", names))
  test <- lasso_list$test[[this_function]]
  train <- lasso_list$train[[this_function]]
  
 res <-  lasso_fit_n_summarize(training_x = train, train_y, testing_x = test, test_y, names)

})

lasso_res %>% as_tibble()


```


Hyper parameter tuning for the best model 'manually', top 50 correlated predictors with the log # of followers + categorical audio features

```{r}
e.grid <- seq(0.001, 0.1, by = 0.0005)
# e.grid <- 10^seq(10,-2,length=100)

# use 5-fold cv
caret_ctrl <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = FALSE,
  savePredictions = TRUE
)
# set seed for reproducibility
set.seed(1993-11-22)

current_time <- Sys.time()

model_cv <- train(x = top_cor_cat_train_x,
  y = train_y,
  method = "glmnet",
  preProc = c("center", "scale"),
  trControl = caret_ctrl,
  tuneGrid = expand.grid(alpha = c(1), lambda = e.grid)
)
end_time <- Sys.time()

end_time - current_time

```

Performance metrics.
```{r}
# model_cv
model_cv$results[model_cv$results$alpha == model_cv$bestTune$alpha & model_cv$results$lambda == model_cv$bestTune$lambda, ]

plot(model_cv)
# lasso_varImp$importance[lasso_varImp$importance$Overall > 0]
```


Let's look at variable importance based on absolute coefficients.
```{r}

coefs <- coef(model_cv$finalModel, model_cv$bestTune$lambda)
model_coefs <- data.frame(preds = rownames(coefs), coefs = coefs[, 1]) %>%
    as_tibble %>% 
    filter(coefs != 0) %>%
    arrange(desc(abs(coefs)))

model_coefs %>% 
  filter(preds != "(Intercept)")   %>% 
  filter(row_number() <= 30) %>% 
  mutate(coefs = abs(coefs)) %>% 
  ggplot(aes(x = reorder(preds, (coefs)), y = (coefs), fill = coefs)) +
    geom_col(show.legend = FALSE) +
    coord_flip() +
    theme_minimal() +
  ggtitle("Lasso - Top 30 Predictors")

```

The most important predictors corroborate with what we saw in the EDA, that is, track popularity, the number of tracks, and artist popularity. The remaining predictors are mostly categorical in nature and include key, key mode, along with highly popular genres and artists found in the the most popular playlists. In contrast, random forest had relatively more quantitative audio features.

```{r}
lasso_cv_p <- model_cv$pred[model_cv$pred$lambda == model_cv$bestTune$lambda & model_cv$pred$alpha == model_cv$bestTune$alpha, ] %>%
  scttrCor(aes(x = pred, y = obs), alpha = 0.8) +
  scale_x_continuous(breaks = seq(0, 17.5, by = 2.5), limits = c(0, 17.5)) + 
  scale_y_continuous(breaks = seq(0, 17.5, by = 2.5), limits = c(0, 17.5)) +
  ggtitle("Lasso Cross-Validation")

lasso_cv_p 
```

```{r, eval = FALSE, include = FALSE}
# predictions using hold-out testing set 

# test_x

lasso_preds <- predict(model_cv, (top_cor_cat_test_x))

test_metrics <- postResample(pred = lasso_preds, obs = test_y)

test_metrics

lasso_val_p <- tibble(pred = lasso_preds, obs = test_y) %>%
  scttrCor(aes(x = pred, y = obs), alpha = 0.7) +
  scale_x_continuous(breaks = seq(0, 17.5, by = 2.5), limits = c(0, 17.5)) + 
  scale_y_continuous(breaks = seq(0, 17.5, by = 2.5), limits = c(0, 17.5)) +
  ggtitle("Lasso Validation")

lasso_val_p
# # residuals
# plot(resid(model_cv))
```


## XGBoost

First, we identify the number of optimal rounds using default values + learning rate of 0.1
```{r}
library(xgboost)
# https://www.slideshare.net/ShangxuanZhang/winning-data-science-competitions-presented-by-owen-zhang
# https://www.slideshare.net/odsc/owen-zhangopen-sourcetoolsanddscompetitions1
# https://machinelearningmastery.com/configure-gradient-boosting-algorithm/

set.seed(1993-11-22)
# https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/

default_params <- list(objective = "reg:linear",
        booster = "gbtree",
        eta = 0.1, #default = 0.3
        gamma = 0 ,
        max_depth = 6, #default=6
        min_child_weight = 1, #default=1
        subsample = 1,
        colsample_bytree = 1# default = 1
)

xgbcv <- xgb.cv( params = default_params, # for finding optimal number of rounds at default parameters
                 data = train_x, label = train_y,
                 nrounds = 999,
                 nfold = 5, 
                 showsd = T, 
                 stratified = T,
                 print_every_n = 40, 
                 early_stopping_rounds = 20, 
                 maximize = F,
                 seed = (1993-11-22))

xgbcv$best_iteration  
# min(xgbcv$test.error.mean)
```
The optimal number of rounds is `r xgbcv$best_iteration`.


Then will identify the best hyperparameter values using 5 fold cross validation with the optimal number of rounds.
```{r, eval = FALSE}

xgb_grid = expand.grid( nrounds = xgbcv$best_iteration,
  eta = c(0.1, 0.05, 0.01),
  max_depth = c(2, 3, 4, 5, 6),
  gamma = 0,
  colsample_bytree= 1,
  min_child_weight=c(1, 2, 3, 4 ,5),
  subsample= 1
)

set.seed(1993-11-22)
current_time <- Sys.time()
xgb_caret <- train(x = train_x, y = train_y,
                   method ='xgbTree', 
                   trControl = caret_ctrl,
                   tuneGrid = xgb_grid)
end_time <- Sys.time()

end_time - current_time # 12 minutes

# saveRDS(xgb_caret, "output/xgb_caret.RDS")

```

```{r}
xgb_caret <- readRDS("output/xgb_caret.RDS")
xgb_caret$bestTune

xgb_caret$results[xgb_caret$results$eta == 0.05 & xgb_caret$results$max_depth == 5 & xgb_caret$results$colsample_bytree == 1 & xgb_caret$results$min_child_weight == 4,]

```


Previously, we found the best number of rounds, but our model has been trained with more rounds than optimalthus before using it for predictions, we should retrain it to search for the optimal number of rounds again using our tuned hyperparameters. (??)

```{r}
set.seed(1993-11-22)
# https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/
optimal_params <- list(
        objective = "reg:linear",
        booster = "gbtree",
        eta = 0.05, #default = 0.3
        gamma = 0 ,
        max_depth = 5, #default=6
        min_child_weight = 4, #default=1
        subsample = 1,
        colsample_bytree = 1# default = 1
)

xgbcv <- xgb.cv( params = optimal_params,  # tuned parameters using optimal parameters
                 data = train_x, label = train_y,
                 nrounds = 999,
                 nfold = 5, 
                 showsd = T, 
                 stratified = T,
                 print_every_n = 40, 
                 early_stopping_rounds = 20, 
                 maximize = F,
                 seed = (1993-11-22))

xgbcv$best_iteration
```

Train the model using the tuned hyperparameters and the optimal number of rounds.
```{r}
# plot(xgb_caret)
#train the model using the best iteration found by cross validation

xgb_mod <- xgb.train(data = xgb.DMatrix(train_x, label = train_y),
                     params = optimal_params, 
                     nrounds = 146, 
                     seed = (1993-11-22))

xgb_mod

xgb_best_cv <- as_tibble(xgb_caret$pred[xgb_caret$pred$eta == 0.05 & xgb_caret$pred$max_depth == 5 & xgb_caret$pred$colsample_bytree == 1 & xgb_caret$pred$min_child_weight == 4,])


xg_cv_p <- xgb_best_cv %>% 
  scttrCor(aes(x = pred, y = obs), alpha = 0.7) + 
    scale_x_continuous(breaks = seq(0, 17.5, by = 2.5), limits = c(0, 17.5)) + 
  scale_y_continuous(breaks = seq(0, 17.5, by = 2.5), limits = c(0, 17.5)) +
  ggtitle("XGBoost Cross-Validation")

xg_cv_p

```

Variable importance

```{r}
library(Ckmeans.1d.dp)
mat <- xgb.importance (feature_names = colnames(train_x), model = xgb_mod)

xgb.ggplot.importance(importance_matrix = mat[1:30], rel_to_first = TRUE)

```



## Comparison of Model Performance Metrics

### Training Data
Based on the training data alone, we can see that the models that best explain the training data are:

4. Ordinary Least Squares
3. Lasso
2. Random Forest
1. XGBoost

```{r}

# CV Metrics
ols_cv <- postResample(spotify_lm$fitted.values, train_y)
rf_cv <- c("RMSE" = sqrt(train_rf$mse[which.min(train_rf$mse)]), 
           "Rsquared" = train_rf$rsq[which.max(train_rf$rsq)], "MAE" = NA)
lasso_cv <- model_cv$results[model_cv$results$alpha == model_cv$bestTune$alpha & model_cv$results$lambda == model_cv$bestTune$lambda, c(3:5) ]
xgb_cv <- xgb_caret$results[xgb_caret$results$eta == 0.05 & xgb_caret$results$max_depth == 5 & xgb_caret$results$colsample_bytree == 1 & xgb_caret$results$min_child_weight == 4, c(8:10)]


data.frame(ols = round(ols_cv, 3),
          rf = round(rf_cv, 3),
          lasso = round(unlist(lasso_cv), 3),
          xgb = round(unlist(xgb_cv), 3))
```

Scatterplots - visualizing predicted vs observed log # of followers.


```{r}
ols_cv_p + rf_cv_p + lasso_cv_p + xg_cv_p + plot_layout(nrow = 2)
```



# Validation Using XGBoost


```{r}

xgb_preds <- predict(xgb_mod, test_x)
postResample(xgb_preds, test_y) 

xg_val_p <- tibble(preds = xgb_preds, obs = test_y) %>% 
  scttrCor(aes(x = preds, y = obs)) +
  ggtitle("XGBoost Validation")

xg_val_p
```



<!-- ### Testing Data -->

<!-- Applying our models to the held-out data, we find that the models that most accurately (based on MAE) predict and explain variation in the log number of followers the best are: -->

<!-- 4. Ordinary Least Squares -->
<!-- 3. Lasso -->
<!-- 2. Random Forest -->
<!-- 1. XGBoost -->

```{r, eval = FALSE, include = FALSE}
# x80_20_split <- data.frame(
#   ols = postResample(ols_preds, test_y),
#   rf = postResample(rf_preds, test_y),
#   lasso = postResample(lasso_preds, test_y),
#   xgb = postResample(xgb_preds, test_y)
# 
# )
# 
x90_10_split <- data.frame(
  ols = postResample(ols_preds, test_y),
  rf = postResample(rf_preds, test_y),
  lasso = postResample(lasso_preds, test_y),
  xgb = postResample(xgb_preds, test_y)

)


# x80_20_split 


x90_10_split


```


```{r, eval = FALSE, include = FALSE}


ols_val_p + rf_val_p + lasso_val_p + xg_val_p + plot_layout(nrow = 2)

```
